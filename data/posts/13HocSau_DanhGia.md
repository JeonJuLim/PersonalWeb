# ๐ง ฤรกnh Giรก Mรด Hรฌnh Hแปc Sรขu (Deep Learning): Hiแปu, ฤo Lฦฐแปng Vร Cแบฃi Thiแปn

> โHuแบฅn luyแปn mรด hรฌnh lร phแบงn dแป, **hiแปu nรณ ฤang hแปc gรฌ vร hแปc tแปt ฤแบฟn ฤรขu** mแปi lร phแบงn khรณ.โ

![Loss Curve Visualization](https://miro.medium.com/v2/resize\:fit:1200/1*o4r7o8vXYyJ7rViJg8pQfw.png)

---

## ๐ I. Giแปi Thiแปu

Khi sinh viรชn bแบฏt ฤแบงu huแบฅn luyแปn cรกc mรด hรฌnh hแปc sรขu (Deep Learning) โ tแปซ mแบกng nฦก-ron (ANN) ฤแบฟn CNN, RNN, hay Transformer โ viแปc **ฤรกnh giรก mรด hรฌnh** lร kแปน nฤng bแบฏt buแปc.

Khรดng chแป xem โฤแป chรญnh xรกcโ (accuracy), bแบกn cแบงn **ฤแปc hiแปu cรกc ฤแป thแป, ma trแบญn, vร thรดng sแป** ฤแป:

* Biแบฟt mรด hรฌnh cรณ ฤang hแปc thแบญt hay chแป ghi nhแป (overfitting).
* Phรกt hiแปn sแปm lแปi trong quรก trรฌnh huแบฅn luyแปn.
* Cแบฃi thiแปn hiแปu suแบฅt mรด hรฌnh qua tuning hoแบทc regularization.

> ๐ง **Theo Feynman:**
> Hรฃy tฦฐแปng tฦฐแปฃng bแบกn lร **huแบฅn luyแปn viรชn bรณng ฤรก** โฝ โ bแบกn khรดng chแป cแบงn biแบฟt tแป sแป trแบญn ฤแบฅu (accuracy), mร cรฒn phแบฃi xem **ฤแปi hรฌnh cรณ cรขn bแบฑng khรดng, cแบงu thแปง cรณ mแปt quรก khรดng, vร chiแบฟn thuแบญt cรณ hแปฃp lรฝ khรดng.**

---

## โ๏ธ II. Loss & Accuracy โ Hai ฤฦฐแปng Cong Quan Trแปng

Mแปi khi bแบกn huแบฅn luyแปn mรด hรฌnh, Colab hay PyTorch ฤแปu hiแปn thแป hai chแป sแป chรญnh:

* **Loss:** sai sแป giแปฏa dแปฑ ฤoรกn vร giรก trแป thแบญt.
* **Accuracy:** tแป lแป dแปฑ ฤoรกn ฤรบng.

### ๐ป Vรญ dแปฅ (Keras)

```python
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20)
```

### ๐ Vแบฝ biแปu ฤแป Loss & Accuracy

```python
import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss qua tแปซng Epoch')
plt.show()
```

> ๐ก **Quan sรกt:**
>
> * Nแบฟu **train loss โ** vร **val loss โ cรนng lรบc** โ mรด hรฌnh hแปc tแปt.
> * Nแบฟu **train loss โ** nhฦฐng **val loss โ** โ mรด hรฌnh ฤang **overfitting**.

![Loss Overfitting Graph](https://miro.medium.com/v2/resize\:fit:1100/1*WNHhUo0aHHMtG0K2WBphMg.png)

> ๐ง **Theo Feynman:**
> Hรฃy tฦฐแปng tฦฐแปฃng bแบกn hแปc thuแปc ฤแป cลฉ ฤแป lรm bรi kiแปm tra.
> ฤiแปm luyแปn tแบญp cao, nhฦฐng thi thแบญt thแบฅp โ bแบกn **hแปc vแบนt** chแปฉ khรดng **hiแปu bรi**.
> ฤรณ chรญnh lร **overfitting.**

---

## ๐งฉ III. Cรกc Thรดng Sแป ฤรกnh Giรก Trong Deep Learning

| Thรดng sแป                          | ร nghฤฉa                        | Mแปฅc tiรชu                               |
| --------------------------------- | ------------------------------ | -------------------------------------- |
| **Training Loss**                 | Sai sแป trรชn dแปฏ liแปu huแบฅn luyแปn | Cรng nhแป cรng tแปt                      |
| **Validation Loss**               | Sai sแป trรชn dแปฏ liแปu kiแปm ฤแปnh  | Cรng nhแป cรng tแปt, khรดng tฤng ฤแปt ngแปt |
| **Accuracy**                      | Tแปท lแป dแปฑ ฤoรกn ฤรบng             | Cรng cao cรng tแปt                      |
| **Precision / Recall / F1-score** | ฤแป chรญnh xรกc vร nhแบกy cแบฃm       | Cรขn bแบฑng khi dแปฏ liแปu mแบฅt cรขn bแบฑng      |
| **AUC-ROC**                       | Khแบฃ nฤng phรขn biแปt giแปฏa 2 lแปp  | Gแบงn 1 lร tแปt                           |

---

## ๐ IV. Ma Trแบญn Nhแบงm Lแบซn Cho Mรด Hรฌnh Deep Learning

Ngay cแบฃ vแปi CNN hay Transformer, bแบกn vแบซn cรณ thแป tแบกo **Confusion Matrix** ฤแป xem lแปi mรด hรฌnh.

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np

y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)

cm = confusion_matrix(y_true, y_pred)
ConfusionMatrixDisplay(cm).plot(cmap='Blues')
```

![Confusion Matrix Example](https://miro.medium.com/v2/resize\:fit:700/1*6G8y2vM_jnZ7ElA2YHfM5w.png)

> ๐ฌ **ร nghฤฉa:**
> Bแบกn biแบฟt mรด hรฌnh **hay nhแบงm lแบซn giแปฏa lแปp nรo** โ vรญ dแปฅ, nhแบงm โmรจoโ vแปi โchรณโ ๐ฑ๐ถ thรฌ cแบงn thรชm dแปฏ liแปu rรต rรng hฦกn.

---

## ๐ง V. Kiแปm Tra Overfitting & Underfitting

| Dแบฅu hiแปu                       | Training | Validation | Kแบฟt luแบญn                               |
| ------------------------------ | -------- | ---------- | -------------------------------------- |
| Loss cao, Accuracy thแบฅp        | Cao      | Cao        | Underfitting (chฦฐa hแปc ฤแปง)             |
| Loss thแบฅp, Accuracy cao        | Thแบฅp     | Cao        | Tแปt (mรด hรฌnh cรขn bแบฑng)                 |
| Loss thแบฅp, nhฦฐng Val Loss tฤng | Thแบฅp     | Tฤng       | Overfitting (hแปc quรก kแปน dแปฏ liแปu train) |

### ๐ง Cรกch khแบฏc phแปฅc Overfitting

* **Regularization:** thรชm `Dropout`, `L2`, hoแบทc `BatchNorm`.
* **Data Augmentation:** xoay, cแบฏt, ฤแบฃo แบฃnh.
* **Early Stopping:** dแปซng huแบฅn luyแปn khi val loss khรดng giแบฃm nแปฏa.

```python
from keras.callbacks import EarlyStopping

callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, callbacks=[callback])
```

> ๐ง **Theo Feynman:**
> Hรฃy tฦฐแปng tฦฐแปฃng bแบกn ฤang luyแปn hรกt ๐ค โ
> Nแบฟu bแบกn chแป hรกt mแปt bรi suแปt 1 thรกng, bแบกn sแบฝ thuแปc lรฒng (overfit).
> Nhฦฐng nแบฟu bแบกn tแบญp nhiแปu bรi, luyแปn hฦกi, luyแปn nhแปp โ bแบกn trแป thรnh **ca sฤฉ thแปฑc thแปฅ (generalize).**

---

## ๐ VI. ฤฦฐแปng Cong Learning Curve & Validation Curve

### 1. Learning Curve โ theo dรตi mรด hรฌnh hแปc theo thแปi gian

```python
from sklearn.model_selection import learning_curve
```

### 2. Validation Curve โ theo dรตi แบฃnh hฦฐแปng cแปงa siรชu tham sแป (hyperparameter)

```python
from sklearn.model_selection import validation_curve
```

> ๐ก Dรนng hai ฤฦฐแปng cong nรy ฤแป **tแปi ฦฐu mรด hรฌnh mร khรดng cแบงn ฤoรกn mรฒ.**

![Learning Curve Example](https://miro.medium.com/v2/resize\:fit:1000/1*BPTF8A9DncZWd2DwU8K62g.png)

---

## ๐งฉ VII. ฤรกnh Giรก Bแบฑng TensorBoard

TensorBoard lร cรดng cแปฅ mแบกnh mแบฝ cแปงa TensorFlow ฤแป trแปฑc quan hรณa quรก trรฌnh huแบฅn luyแปn.

```python
from tensorflow.keras.callbacks import TensorBoard

tensorboard = TensorBoard(log_dir='./logs')
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, callbacks=[tensorboard])
```

Chแบกy TensorBoard:

```bash
!tensorboard --logdir ./logs
```

> ๐ Xem biแปu ฤแป loss, accuracy, graph mรด hรฌnh, histogram cแปงa tham sแป vร learning rate.

---

## ๐งญ VIII. Tแปng Kแบฟt

| Chแป sแป                    | ร nghฤฉa                     | Cรกch ฤแปc                         |
| ------------------------- | --------------------------- | -------------------------------- |
| **Train Loss / Val Loss** | Sai sแป hแปc vร kiแปm ฤแปnh     | Nแบฟu chรชnh lแปch lแปn โ overfitting |
| **Accuracy / F1-score**   | ฤแป chรญnh xรกc dแปฑ ฤoรกn        | Cรng cao cรng tแปt                |
| **Confusion Matrix**      | Phรขn tรญch lแปi tแปซng lแปp      | Dรนng cho phรขn loแบกi               |
| **ROC-AUC**               | ฤรกnh giรก khแบฃ nฤng phรขn biแปt | Gแบงn 1 lร tแปt                     |
| **Learning Curve**        | Theo dรตi quรก trรฌnh hแปc      | Giรบp chแปn epoch tแปi ฦฐu           |
| **TensorBoard**           | Quan sรกt toรn bแป quรก trรฌnh  | Trแปฑc quan, chuyรชn nghiแปp         |

---

> ๐ฌ **Kแบฟt luแบญn:**
> Mแปt sinh viรชn hแปc mรกy giแปi khรดng chแป biแบฟt โtrain mรด hรฌnhโ mร cรฒn biแบฟt **ฤแปc mรด hรฌnh nhฦฐ ฤแปc nhแบกc phแป** ๐ผ.
> Mแปi chแป sแป lร **mแปt nแปt nhแบกc** โ hiแปu ฤฦฐแปฃc chรบng, bแบกn sแบฝ ฤiแปu khiแปn โbแบฃn giao hฦฐแปng AIโ cแปงa mรฌnh thแบญt hoรn hแบฃo.
