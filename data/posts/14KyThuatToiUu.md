# âš™ï¸ Ká»¹ Thuáº­t Tá»‘i Æ¯u ThÃ´ng Sá»‘ (Hyperparameter Optimization) Trong Há»c MÃ¡y

> â€œHuáº¥n luyá»‡n mÃ´ hÃ¬nh lÃ  nghá»‡ thuáº­t â€” cÃ²n tá»‘i Æ°u tham sá»‘ lÃ  khoa há»c.â€ ğŸ§ 

![Hyperparameter Tuning](https://miro.medium.com/v2/resize\:fit:1200/1*VMTc3aK7FQDL4tJ8u4b5bQ.png)

---

## ğŸ§­ I. Giá»›i Thiá»‡u

Trong há»c mÃ¡y, **hyperparameters** lÃ  nhá»¯ng â€œnÃºt Ä‘iá»u chá»‰nhâ€ bÃªn ngoÃ i mÃ´ hÃ¬nh â€” báº¡n **khÃ´ng há»c Ä‘Æ°á»£c chÃºng tá»« dá»¯ liá»‡u**, mÃ  **pháº£i chá»n thá»§ cÃ´ng hoáº·c dÃ¹ng thuáº­t toÃ¡n tá»‘i Æ°u**.

VÃ­ dá»¥:

* Há»c mÃ¡y cá»• Ä‘iá»ƒn: sá»‘ cÃ¢y trong Random Forest, k trong KNN.
* Há»c sÃ¢u: learning rate, batch size, sá»‘ layer, sá»‘ neuron.

> ğŸ’¡ **Theo Feynman:**
> HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang **nÆ°á»›ng bÃ¡nh** ğŸ â€” báº¡n khÃ´ng há»c cÃ´ng thá»©c tá»« bá»™t, mÃ  báº¡n pháº£i **tá»± thá»­ nhiá»‡t Ä‘á»™, thá»i gian, vÃ  lÆ°á»£ng Ä‘Æ°á»ng** Ä‘á»ƒ ra chiáº¿c bÃ¡nh hoÃ n háº£o. ÄÃ³ chÃ­nh lÃ  tinh tháº§n cá»§a hyperparameter tuning.

---

## ğŸ”¢ II. PhÃ¢n Loáº¡i Hyperparameter Theo MÃ´ HÃ¬nh

| Loáº¡i mÃ´ hÃ¬nh                            | ThÃ´ng sá»‘ cáº§n tá»‘i Æ°u                           | áº¢nh hÆ°á»Ÿng                     |
| --------------------------------------- | --------------------------------------------- | ----------------------------- |
| **Há»“i quy / PhÃ¢n loáº¡i (Logistic, SVM)** | Regularization (C), kernel, gamma             | Kiá»ƒm soÃ¡t overfitting         |
| **CÃ¢y quyáº¿t Ä‘á»‹nh / Random Forest**      | Sá»‘ cÃ¢y (n_estimators), Ä‘á»™ sÃ¢u (max_depth)     | Äá»™ khÃ¡i quÃ¡t cá»§a mÃ´ hÃ¬nh      |
| **KNN**                                 | Sá»‘ lÃ¡ng giá»ng (k), khoáº£ng cÃ¡ch (metric)       | CÃ¢n báº±ng Ä‘á»™ mÆ°á»£t vÃ  chÃ­nh xÃ¡c |
| **Neural Network / Deep Learning**      | Learning rate, batch size, dropout, optimizer | Tá»‘c Ä‘á»™ vÃ  kháº£ nÄƒng há»c        |

---

## âš™ï¸ III. Ká»¹ Thuáº­t Tá»‘i Æ¯u Hyperparameter

### ğŸ”¹ 1. Grid Search â€“ TÃ¬m kiáº¿m toÃ n bá»™ lÆ°á»›i tham sá»‘

Thá»­ **má»i tá»• há»£p cÃ³ thá»ƒ** trong táº­p giÃ¡ trá»‹ cho trÆ°á»›c.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15]
}

grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)
grid.fit(X_train, y_train)

print(grid.best_params_)
```

> âœ… Æ¯u Ä‘iá»ƒm: TÃ¬m Ä‘Æ°á»£c cáº¥u hÃ¬nh tá»‘i Æ°u.
> âš ï¸ NhÆ°á»£c Ä‘iá»ƒm: Tá»‘n thá»i gian, Ä‘áº·c biá»‡t vá»›i nhiá»u tham sá»‘.

---

### ğŸ”¹ 2. Random Search â€“ Thá»­ ngáº«u nhiÃªn trong khÃ´ng gian tham sá»‘

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(2, 20)
}

random_search = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=10)
random_search.fit(X_train, y_train)

print(random_search.best_params_)
```

> âœ… Æ¯u Ä‘iá»ƒm: Nhanh, hiá»‡u quáº£ vá»›i khÃ´ng gian lá»›n.
> âš ï¸ NhÆ°á»£c Ä‘iá»ƒm: CÃ³ thá»ƒ bá» lá»¡ vÃ¹ng tá»‘i Æ°u.

> ğŸ§  **Theo Feynman:**
> Giá»‘ng nhÆ° **báº¡n thá»­ nÆ°á»›ng bÃ¡nh á»Ÿ cÃ¡c nhiá»‡t Ä‘á»™ ngáº«u nhiÃªn khÃ¡c nhau**, Ä‘Ã´i khi láº¡i tÃ¬m ra â€œbÃ­ kÃ­p gia truyá»nâ€ khÃ´ng ngá» tá»›i.

---

### ğŸ”¹ 3. Bayesian Optimization â€“ Tá»‘i Æ°u thÃ´ng minh dá»±a trÃªn xÃ¡c suáº¥t

DÃ¹ng mÃ´ hÃ¬nh thá»‘ng kÃª (Gaussian Process) Ä‘á»ƒ **dá»± Ä‘oÃ¡n vÃ¹ng tham sá»‘ tiá»m nÄƒng**, sau Ä‘Ã³ chá»‰ thá»­ á»Ÿ vÃ¹ng Ä‘Ã³.

```python
from skopt import BayesSearchCV

search_spaces = {
    'C': (1e-6, 1e+6, 'log-uniform'),
    'gamma': (1e-6, 1e+1, 'log-uniform')
}

opt = BayesSearchCV(SVC(), search_spaces, n_iter=32, cv=3)
opt.fit(X_train, y_train)
print(opt.best_params_)
```

> âœ… Æ¯u Ä‘iá»ƒm: Tiáº¿t kiá»‡m thá»i gian, hiá»‡u quáº£ cao.
> âš ï¸ NhÆ°á»£c Ä‘iá»ƒm: CÃ i Ä‘áº·t phá»©c táº¡p hÆ¡n, cáº§n thÆ° viá»‡n ngoÃ i (`scikit-optimize`).

> ğŸ§  **Theo Feynman:**
> Giá»‘ng nhÆ° báº¡n **há»c tá»« kinh nghiá»‡m nÆ°á»›ng bÃ¡nh láº§n trÆ°á»›c**, thay vÃ¬ thá»­ láº¡i tá»« Ä‘áº§u.

---

### ğŸ”¹ 4. Hyperband & Optuna â€“ Thá»­ nhanh, dá»«ng sá»›m

#### ğŸ“˜ Optuna Example

```python
import optuna

def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 50, 300)
    max_depth = trial.suggest_int('max_depth', 2, 15)

    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)
    clf.fit(X_train, y_train)
    return clf.score(X_val, y_val)

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

print(study.best_params)
```

> âœ… Æ¯u Ä‘iá»ƒm: Dá»«ng sá»›m mÃ´ hÃ¬nh yáº¿u, táº­p trung tÃ i nguyÃªn cho mÃ´ hÃ¬nh tá»‘t.
> âš ï¸ NhÆ°á»£c Ä‘iá»ƒm: Cáº§n GPU/CPU Ä‘á»§ máº¡nh náº¿u thá»­ nhiá»u mÃ´ hÃ¬nh song song.

---

## ğŸ“ˆ IV. CÃ¡c ThÃ´ng Sá»‘ Cáº§n Tá»‘i Æ¯u Trong Deep Learning

| ThÃ´ng sá»‘          | MÃ´ táº£                                 | Gá»£i Ã½ tá»‘i Æ°u                                             |
| ----------------- | ------------------------------------- | -------------------------------------------------------- |
| **Learning Rate** | BÆ°á»›c nháº£y khi cáº­p nháº­t trá»ng sá»‘       | Thá»­ theo log-scale: 1e-1 â†’ 1e-5                          |
| **Batch Size**    | Sá»‘ máº«u cáº­p nháº­t má»—i láº§n               | Nhá» (16-64): á»•n Ä‘á»‹nh; Lá»›n (128+): nhanh nhÆ°ng dá»… overfit |
| **Epochs**        | Sá»‘ vÃ²ng láº·p huáº¥n luyá»‡n                | Dá»«ng sá»›m báº±ng EarlyStopping                              |
| **Dropout**       | XÃ³a ngáº«u nhiÃªn neuron Ä‘á»ƒ giáº£m overfit | 0.2 â€“ 0.5 lÃ  phá»• biáº¿n                                    |
| **Optimizer**     | CÃ¡ch tá»‘i Æ°u trá»ng sá»‘                  | Adam, SGD, RMSprop â€“ thá»­ nhiá»u Ä‘á»ƒ so sÃ¡nh                |

### ğŸ” Máº¹o Thá»±c Nghiá»‡m:

* DÃ¹ng **Learning Rate Finder** (`lr_find` trong fastai).
* Váº½ **Loss vs Learning Rate** Ä‘á»ƒ chá»n vÃ¹ng â€œá»•n Ä‘á»‹nh vÃ  giáº£m nhanhâ€.

> ğŸ§  **Theo Feynman:**
> Learning rate giá»‘ng nhÆ° **bÆ°á»›c chÃ¢n cá»§a báº¡n trÃªn nÃºi** ğŸ”ï¸ â€”
> BÆ°á»›c quÃ¡ to thÃ¬ váº¥p ngÃ£ (diverge), bÆ°á»›c quÃ¡ nhá» thÃ¬ khÃ´ng bao giá» lÃªn Ä‘á»‰nh.

---

## ğŸ§© V. ÄÃ¡nh GiÃ¡ Sau Khi Tá»‘i Æ¯u

DÃ¹ng **cross-validation** Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ á»•n Ä‘á»‹nh.

```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(grid.best_estimator_, X, y, cv=5)
print(f"Accuracy trung bÃ¬nh: {scores.mean():.3f} Â± {scores.std():.3f}")
```

> ğŸ’¬ Káº¿t quáº£ khÃ´ng chá»‰ cáº§n **cao**, mÃ  cÃ²n pháº£i **á»•n Ä‘á»‹nh giá»¯a cÃ¡c láº§n cháº¡y**.

---

## ğŸ§­ VI. Tá»•ng Káº¿t

| Ká»¹ thuáº­t                  | Khi nÃªn dÃ¹ng                | Ghi chÃº              |
| ------------------------- | --------------------------- | -------------------- |
| **Grid Search**           | KhÃ´ng gian nhá»              | ToÃ n diá»‡n nhÆ°ng cháº­m |
| **Random Search**         | KhÃ´ng gian lá»›n              | Nhanh, dá»… triá»ƒn khai |
| **Bayesian Optimization** | Cáº§n tá»‘i Æ°u thÃ´ng minh       | Hiá»‡u quáº£ cao         |
| **Optuna / Hyperband**    | Deep learning, mÃ´ hÃ¬nh náº·ng | Dá»«ng sá»›m mÃ´ hÃ¬nh yáº¿u |

---

> ğŸ’¬ **Káº¿t luáº­n:**
> Tá»‘i Æ°u hyperparameter khÃ´ng chá»‰ lÃ  â€œcháº¡y nhiá»u láº§nâ€ â€” mÃ  lÃ  **hiá»ƒu mÃ´ hÃ¬nh, hiá»ƒu dá»¯ liá»‡u, vÃ  chá»n hÆ°á»›ng thÃ´ng minh**.
> Má»™t sinh viÃªn giá»i Há»c mÃ¡y khÃ´ng tÃ¬m ngáº«u nhiÃªn, mÃ  **tÃ¬m cÃ³ chiáº¿n lÆ°á»£c vÃ  lÃ½ luáº­n.**
